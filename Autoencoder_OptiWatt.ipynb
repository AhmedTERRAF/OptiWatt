{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75280d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your converted CSV file\n",
    "df = pd.read_csv(\"household_power_consumption.csv\")\n",
    "\n",
    "# Display first few rows and column types\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('household_power_consumption.csv')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c04d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"household_power_consumption.csv\")\n",
    "\n",
    "# Combine Date and Time into datetime format\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Drop original Date and Time columns\n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "\n",
    "# Numeric columns to convert\n",
    "numeric_columns = [\n",
    "    'Global_active_power', 'Global_reactive_power', 'Voltage',\n",
    "    'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'\n",
    "]\n",
    "\n",
    "# Convert to numeric (float)\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84667a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Handle missing data (interpolation recommended for time-series)\n",
    "df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Confirm no missing values remain\n",
    "print(\"Remaining missing values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb01e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_columns = [\n",
    "    'Global_active_power', 'Global_reactive_power', 'Voltage',\n",
    "    'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'\n",
    "]\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df[numeric_columns].corr()\n",
    "\n",
    "# Display correlation matrix\n",
    "print(\"Correlation matrix:\\n\", corr_matrix)\n",
    "\n",
    "# Visualization of correlation\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadf3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant column\n",
    "df.drop('Global_intensity', axis=1, inplace=True)\n",
    "\n",
    "# Verify resulting columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is already loaded, preprocessed, redundancy handled\n",
    "\n",
    "# CO₂ emission factor for electricity in France\n",
    "emission_factor_france = 0.053  \n",
    "\n",
    "# Convert Wh to kWh and calculate CO₂ emissions for each submeter\n",
    "for col in ['Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']:\n",
    "    df[f'{col}_kWh'] = df[col] / 1000  # Wh to kWh\n",
    "    df[f'{col}_CO2_kg'] = df[f'{col}_kWh'] * emission_factor_france\n",
    "\n",
    "# Define global columns explicitly\n",
    "global_columns = ['DateTime', 'Global_active_power', 'Global_reactive_power', 'Voltage']\n",
    "\n",
    "# Create separate, clearly structured DataFrames per submeter without other submeters' data\n",
    "submeter_data = {}\n",
    "\n",
    "# Sub_metering_1 dataset\n",
    "submeter_data['Sub_metering_1'] = df[global_columns + [\n",
    "    'Sub_metering_1', 'Sub_metering_1_kWh', 'Sub_metering_1_CO2_kg'\n",
    "]].copy().rename(columns={\n",
    "    'Sub_metering_1': 'Consumption_Wh',\n",
    "    'Sub_metering_1_kWh': 'Consumption_kWh',\n",
    "    'Sub_metering_1_CO2_kg': 'CO2_kg'\n",
    "})\n",
    "\n",
    "# Sub_metering_2 dataset\n",
    "submeter_data['Sub_metering_2'] = df[global_columns + [\n",
    "    'Sub_metering_2', 'Sub_metering_2_kWh', 'Sub_metering_2_CO2_kg'\n",
    "]].copy().rename(columns={\n",
    "    'Sub_metering_2': 'Consumption_Wh',\n",
    "    'Sub_metering_2_kWh': 'Consumption_kWh',\n",
    "    'Sub_metering_2_CO2_kg': 'CO2_kg'\n",
    "})\n",
    "\n",
    "# Sub_metering_3 dataset\n",
    "submeter_data['Sub_metering_3'] = df[global_columns + [\n",
    "    'Sub_metering_3', 'Sub_metering_3_kWh', 'Sub_metering_3_CO2_kg'\n",
    "]].copy().rename(columns={\n",
    "    'Sub_metering_3': 'Consumption_Wh',\n",
    "    'Sub_metering_3_kWh': 'Consumption_kWh',\n",
    "    'Sub_metering_3_CO2_kg': 'CO2_kg'\n",
    "})\n",
    "\n",
    "# Verify clearly each resulting dataset\n",
    "for submeter, data in submeter_data.items():\n",
    "    print(f\"\\n--- Clearly structured data for {submeter} ---\")\n",
    "    print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95841ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Dictionary to store scalers for each appliance (used for inverse transform later)\n",
    "scalers = {}\n",
    "\n",
    "# Normalize each submeter's dataset independently\n",
    "for submeter, df_appliance in submeter_data.items():\n",
    "    # Select only numeric feature columns (excluding DateTime)\n",
    "    feature_cols = df_appliance.columns.drop('DateTime')\n",
    "    \n",
    "    # Initialize and apply MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = df_appliance.copy()\n",
    "    df_scaled[feature_cols] = scaler.fit_transform(df_appliance[feature_cols])\n",
    "    \n",
    "    # Replace original with scaled version\n",
    "    submeter_data[submeter] = df_scaled\n",
    "    \n",
    "    # Save scaler for future inverse transform (visualization or deployment)\n",
    "    scalers[submeter] = scaler\n",
    "    \n",
    "    print(f\" Normalized data for {submeter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define sequence length\n",
    "timesteps = 60  # 1 hour if data is sampled every minute\n",
    "\n",
    "# Create sequences per submeter\n",
    "appliance_sequences = {}\n",
    "\n",
    "for submeter, df_scaled in submeter_data.items():\n",
    "    feature_cols = df_scaled.columns.drop('DateTime')\n",
    "    data_array = df_scaled[feature_cols].values\n",
    "\n",
    "    # Create rolling window sequences\n",
    "    sequences = []\n",
    "    for i in range(len(data_array) - timesteps):\n",
    "        window = data_array[i:i + timesteps]\n",
    "        sequences.append(window)\n",
    "\n",
    "    appliance_sequences[submeter] = np.array(sequences)\n",
    "    print(f\" Generated {len(sequences)} sequences for {submeter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store split data per appliance\n",
    "train_test_split_data = {}\n",
    "\n",
    "# Define train/test ratio\n",
    "train_fraction = 0.8\n",
    "\n",
    "for submeter, sequences in appliance_sequences.items():\n",
    "    n = len(sequences)\n",
    "    train_size = int(n * train_fraction)\n",
    "\n",
    "    # No shuffling: chronological split\n",
    "    X_train = sequences[:train_size]\n",
    "    X_test = sequences[train_size:]\n",
    "\n",
    "    train_test_split_data[submeter] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test\n",
    "    }\n",
    "\n",
    "    print(f\" {submeter} — Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "\n",
    "# Optimized model (fewer units, simple structure)\n",
    "def create_autoencoder(timesteps, n_features):\n",
    "    input_layer = Input(shape=(timesteps, n_features))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = LSTM(32, activation='relu', return_sequences=False)(input_layer)\n",
    "    \n",
    "    # Repeat vector to match input shape\n",
    "    repeated = RepeatVector(timesteps)(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = LSTM(32, activation='relu', return_sequences=True)(repeated)\n",
    "    \n",
    "    # Output\n",
    "    output = TimeDistributed(Dense(n_features))(decoded)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Stop training if no improvement for 3 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoders = {}\n",
    "history_logs = {}\n",
    "\n",
    "for submeter in train_test_split_data:\n",
    "    X_train = train_test_split_data[submeter]['X_train']\n",
    "    \n",
    "    print(f\"\\n Training Autoencoder for {submeter}...\")\n",
    "\n",
    "    # Create model\n",
    "    model = create_autoencoder(timesteps=X_train.shape[1], n_features=X_train.shape[2])\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(\n",
    "        X_train, X_train,                  # Autoencoder input = output\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Save model and history\n",
    "    autoencoders[submeter] = model\n",
    "    history_logs[submeter] = history\n",
    "    \n",
    "    print(f\" Finished training {submeter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76177836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder to store the models\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save each model individually\n",
    "for submeter, model in autoencoders.items():\n",
    "    model_path = os.path.join(save_dir, f\"{submeter}_autoencoder.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\" Model for {submeter} saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d41636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Folder to store scalers\n",
    "scaler_dir = \"saved_scalers\"\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "\n",
    "# Save each scaler per submeter\n",
    "for submeter, scaler in scalers.items():\n",
    "    scaler_path = os.path.join(scaler_dir, f\"{submeter}_scaler.joblib\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\" Scaler for {submeter} saved to: {scaler_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41330d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for submeter, history in history_logs.items():\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f\" Loss Curve — {submeter}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd224d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "reconstruction_errors = {}\n",
    "for submeter in train_test_split_data:\n",
    "    print(f\"\\n Detecting anomalies for {submeter}...\")\n",
    "\n",
    "    # Get model and test data\n",
    "    model = autoencoders[submeter]\n",
    "    X_test = train_test_split_data[submeter]['X_test']\n",
    "\n",
    "    # Predict (reconstruct) test sequences\n",
    "    X_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute MSE between original and reconstructed sequence\n",
    "    mse = np.mean(np.power(X_test - X_pred, 2), axis=(1, 2))  # one error value per sequence\n",
    "    reconstruction_errors[submeter] = mse\n",
    "\n",
    "    print(f\" Reconstruction errors for {submeter}: mean={mse.mean():.5f}, std={mse.std():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_thresholds = {}\n",
    "anomalies = {}\n",
    "\n",
    "for submeter, mse in reconstruction_errors.items():\n",
    "    # Define threshold (can be tuned!)\n",
    "    threshold = np.percentile(mse, 95)\n",
    "    anomaly_thresholds[submeter] = threshold\n",
    "\n",
    "    # Flag as anomaly if reconstruction error exceeds threshold\n",
    "    anomaly_flags = mse > threshold\n",
    "    anomalies[submeter] = anomaly_flags\n",
    "\n",
    "    print(f\" {submeter} — Anomaly threshold: {threshold:.5f}, Total anomalies detected: {np.sum(anomaly_flags)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b606bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for submeter, mse in reconstruction_errors.items():\n",
    "    threshold = anomaly_thresholds[submeter]\n",
    "    flags = anomalies[submeter]\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(mse, label='Reconstruction Error')\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', label='Anomaly Threshold')\n",
    "    plt.title(f' Anomaly Detection — {submeter}')\n",
    "    plt.xlabel('Test Sequence Index')\n",
    "    plt.ylabel('MSE (Reconstruction Error)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for submeter, errors in reconstruction_errors.items():\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(errors, bins=50, color='steelblue')\n",
    "    plt.axvline(x=np.percentile(errors, 95), color='red', linestyle='--', label='Threshold (95th percentile)')\n",
    "    plt.title(f\" Error Distribution — {submeter}\")\n",
    "    plt.xlabel(\"Reconstruction Error (MSE)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select the submeter you want to visualize\n",
    "sub = 'Sub_metering_3'\n",
    "\n",
    "# Grab full normalized dataframe\n",
    "df = submeter_data[sub]\n",
    "\n",
    "# Get test reconstruction error info\n",
    "errors = reconstruction_errors[sub]\n",
    "flags = anomalies[sub]\n",
    "threshold = anomaly_thresholds[sub]\n",
    "\n",
    "# 1. Where does the test set start in the original dataframe?\n",
    "test_start_idx = int(len(df) * 0.8)\n",
    "\n",
    "# 2. Number of test sequences = number of reconstruction errors\n",
    "num_test_sequences = len(errors)\n",
    "\n",
    "# 3. The first usable sequence starts at test_start_idx + timesteps\n",
    "# So we compute indices that map each test sequence to its last time step in df\n",
    "sequence_indices = np.arange(test_start_idx + timesteps, test_start_idx + timesteps + num_test_sequences)\n",
    "\n",
    "# 4. Make sure these indices do not exceed the length of the dataframe\n",
    "valid_mask = sequence_indices < len(df)\n",
    "sequence_indices = sequence_indices[valid_mask]\n",
    "flags = flags[valid_mask]\n",
    "\n",
    "# 5. Extract the actual consumption and timestamps from df\n",
    "consumption = df.iloc[sequence_indices]['Consumption_Wh'].values\n",
    "timestamps = df.iloc[sequence_indices]['DateTime'].values\n",
    "\n",
    "# 6. Plot everything\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(timestamps, consumption, label='Consumption (Wh)', linewidth=1)\n",
    "\n",
    "# Highlight anomalies in red\n",
    "plt.scatter(timestamps[flags], consumption[flags], color='red', label='Anomalies', s=30)\n",
    "\n",
    "plt.title(f\"⏱ Anomaly Detection on Test Set — {sub}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Consumption (Wh)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "sub = 'Sub_metering_1'\n",
    "df = submeter_data[sub].copy()\n",
    "errors = reconstruction_errors[sub]\n",
    "flags = anomalies[sub]\n",
    "\n",
    "# Align test data\n",
    "test_start_idx = int(len(df) * 0.8)\n",
    "sequence_indices = np.arange(test_start_idx + timesteps, test_start_idx + timesteps + len(errors))\n",
    "valid_mask = sequence_indices < len(df)\n",
    "sequence_indices = sequence_indices[valid_mask]\n",
    "timestamps = df.iloc[sequence_indices]['DateTime'].values\n",
    "consumption = df.iloc[sequence_indices]['Consumption_Wh'].values\n",
    "errors = errors[valid_mask]\n",
    "flags = flags[valid_mask]\n",
    "\n",
    "# Build plotly traces\n",
    "trace1 = go.Scatter(x=timestamps, y=consumption, name='Consumption (Wh)', line=dict(color='blue'))\n",
    "trace2 = go.Scatter(x=timestamps[flags], y=consumption[flags], mode='markers', name='Anomalies', marker=dict(color='red', size=6))\n",
    "trace3 = go.Scatter(x=timestamps, y=errors, name='Reconstruction Error', yaxis='y2', line=dict(color='orange'))\n",
    "\n",
    "# Layout\n",
    "layout = go.Layout(\n",
    "    title=f' Interactive Anomaly Detection — {sub}',\n",
    "    xaxis=dict(title='Timestamp'),\n",
    "    yaxis=dict(title='Consumption (Wh)', side='left'),\n",
    "    yaxis2=dict(title='Reconstruction Error (MSE)', overlaying='y', side='right'),\n",
    "    legend=dict(x=0, y=1.1, orientation='h'),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
